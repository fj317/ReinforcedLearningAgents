{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b22fffdf3b2aca8279ddb7db3469abc1",
     "grade": false,
     "grade_id": "cell-b0b3b6b4f891b031",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Graded Assessment 2: Racetrack\n",
    "\n",
    "In this assignment, you will implement and compare the performance of three reinforcement learning algorithms: On-Policy First-Visit Monte-Carlo Control, Sarsa, and Q-Learning.\n",
    "\n",
    "**Total number of marks:** 40 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include **all** of your source code. Please **do not change the file name or compress/zip your submission**. Please do not include any identifying information on the files you submit. This coursework will be marked **anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and **write your own code**.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, please **ensure that you follow these instructions**. Not doing so will cause the our marking software to reject your work, and will assign you a score of zero for that question. Please **do not duplicate or delete existing cells**: if you need additional cells, please insert new ones. **If our marking software rejects your work because you have not followed our instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please **do not use any non-standard, third-party libraries** apart from numpy and matplotlib. In this assignment, you should also use the `racetrack_env.py` file, which we have provided for you. **If we are unable to run your code because you have used unsupported external libraries, you may not get any credit for your work.**\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel → Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **10 minutes** on the University's lab computers, and **written answer length limits** must be adhered to. Otherwise, you may not get credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d61de96551405090cfd026321fcacb3d",
     "grade": false,
     "grade_id": "cell-e86e35a4b405ff32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Racetrack Environment\n",
    "We have implemented a custom environment called \"Racetrack\" for you to use during this piece of coursework. It is inspired by the environment described in the course textbook (Reinforcement Learning, Sutton & Barto, 2018, Exercise 5.12), but is not exactly the same.\n",
    "\n",
    "### Environment Description\n",
    "Consider driving a race car around a turn on a racetrack. In order to complete the race as quickly as possible, you would want to drive as fast as you can but, to avoid running off the track, you must slow down while turning.\n",
    "\n",
    "In our simplified racetrack environment, the agent is at one of a discrete set of grid positions. The agent also has a discrete speed in two directions, $x$ and $y$. So the state is represented as follows:\n",
    "$$(\\text{position}_y, \\text{position}_x, \\text{velocity}_y, \\text{velocity}_x)$$\n",
    "\n",
    "The agent collects a reward of -1 at each time step, an additional -10 for leaving the track (i.e., ending up on a black grid square in the figure below), and an additional +10 for reaching the finish line (any of the red grid squares). The agent starts each episode on a randomly selected grid-square on the starting line (green grid squares) with a speed of zero in both directions. At each time step, the agent can change its speed in both directions. Each speed can be changed by +1, -1 or 0, giving a total of nine actions. For example, the agent may increase its speed in the $x$ direction by -1 and its speed in the $y$ direction by +1. The agent's speed cannot be greater than +10 or less than -10 in either direction.\n",
    "\n",
    "<img src=\"images/track_big.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "The agent's next state is determined by its current grid square, its current speed in two directions, and the changes it  makes to its speed in the two directions. This environment is stochastic. When the agent tries to change its speed, no change occurs (in either direction) with probability 0.2. In other words, 20% of the time, the agent's action is ignored and the car's speed remains the same in both directions.\n",
    "\n",
    "If the agent leaves the track, it is returned to a random start grid-square and has its speed set to zero in both directions; the episode continues. An episode ends only when the agent transitions to a goal grid-square.\n",
    "\n",
    "\n",
    "\n",
    "### Environment Implementation\n",
    "We have implemented the above environment in the `racetrack_env.py` file, for you to use in this coursework. Please use this implementation instead of writing your own, and please do not modify the environment.\n",
    "\n",
    "We provide a `RacetrackEnv` class for your agents to interact with. The class has the following methods:\n",
    "- **`reset()`** - this method initialises the environment, chooses a random starting state, and returns it. This method should be called before the start of every episode.\n",
    "- **`step(action)`** - this method takes an integer action (more on this later), and executes one time-step in the environment. It returns a tuple containing the next state, the reward collected, and whether the next state is a terminal state.\n",
    "- **`render(sleep_time)`** - this method renders a matplotlib graph representing the environment. It takes an optional float parameter giving the number of seconds to display each time-step. This method is useful for testing and debugging, but should not be used during training since it is *very* slow. **Do not use this method in your final submission**.\n",
    "- **`get_actions()`** - a simple method that returns the available actions in the current state. Always returns a list containing integers in the range [0-8] (more on this later).\n",
    "\n",
    "In our code, states are represented as Python tuples - specifically a tuple of four integers. For example, if the agent is in a grid square with coordinates ($Y = 2$, $X = 3$), and is moving zero cells vertically and one cell horizontally per time-step, the state is represented as `(2, 3, 0, 1)`. Tuples of this kind will be returned by the `reset()` and `step(action)` methods. It is worth noting that tuples can be used to index certain Python data-structures, such as dictionaries.\n",
    "\n",
    "There are nine actions available to the agent in each state, as described above. However, to simplify your code, we have represented each of the nine actions as an integer in the range [0-8]. The table below shows the index of each action, along with the corresponding changes it will cause to the agent's speed in each direction.\n",
    "\n",
    "<img src=\"images/action_grid.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "For example, taking action 8 will increase the agent's speed in the $x$ direction, but decrease its speed in the $y$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4714ee739adca912176564b3eb00229",
     "grade": false,
     "grade_id": "cell-30ac99abe97e62b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Racetrack Code Example\n",
    "Below, we go through a quick example of using the `RaceTrackEnv` class.\n",
    "\n",
    "First, we import the class, then create a `RaceTrackEnv` object called `env`. We then initialise the environment using the `reset()` method, and take a look at the initial state variable and the result of `plot()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac22a56ca4687400306302c35b75a91",
     "grade": false,
     "grade_id": "cell-77add459a6f282dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAADrCAYAAAAv8oAHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEUklEQVR4nO3dwY0UVxRA0SpECM6BAMiJOIjDMXnpMJwAq2IDyAuPXFXdQ93XdY7EApivN6ur39L8N+u2bQtAxYervwGAfxMlIEWUgBRRAlJECUgRJSDl4/99wbquX5Zl+fLjr5/f99sB7mLbtvW//n098nNK67r6oSbgKd6Kko9vQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkHJ0SwDAu7Il4OaO/jabdV1PnXvk7KUzD09clp9P34+ePXvu6pln2RIAjCBKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpHiQC1zirQe5VpcAKW5KIaNWetxl5uGJVpfsZXUJMIIoASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpHj7BlzClgBgBDelJ7vN6/m7zDw80ZaAvWwJAEYQJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlI8yAUuYXUJMIKb0htGrdcw8/1mHp5odcleVpcAI4gSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAirdvwCVsCQBGeOmb0m1esl8wEx5lSwAwgigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQ8tIPcoEuq0uAEV76pnSXNSKPzISrWF0CjCBKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKS/99g3osiUAGGHETekuL/bPzoSJbAkARhAlIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkY8yAVej9UlwAgjbkqT1og8MhPuxOoSYARRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBnx9g14PbYEACP8tpvStBf7Z2cC+9gSAIwgSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKRYXQJcwuoSYASrS548E9jH6hJgBFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIsSUAuIQtAcAItgQ8eSawjy0BwAiiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIrVJcAlrC4BRrC65MkzgX2sLgFGECUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUWwKAS9gSAIzw+25KXx84c/Tsj68/sSRg+fXY/+RMYB9bAoARRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUq0uAS1hdAozgpgRcwuoSYARRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIOXoloBvy7L8fWLOH8uy/HPi3CNnzTTTzO7MT2/+z7Ztu/8sy/LXka9/9JyZZpp5v5k+vgEpogSkHI3SnyfnnD1npplm3mzmoc2TAO/NxzcgRZSAFFECUkQJSBElIOU7NU0XohVzxiQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (1, 3, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set random seed to make example reproducable.\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "from racetrack_env import RacetrackEnv\n",
    "\n",
    "# Instantiate environment object.\n",
    "env = RacetrackEnv()\n",
    "\n",
    "# Initialise/reset environment.\n",
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Initial State: {}\".format(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf70e6e3c9fe761473c11366c91f40ff",
     "grade": false,
     "grade_id": "cell-b42bead8118e3c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, `reset()` has returned a valid initial state as a four-tuple. The function `plot()` uses the same colour-scheme as described above, but also includes a yellow grid-square to indicate the current position of the agent.\n",
    "\n",
    "Let's make the agent go upward by using `step(1)`, then inspect the result (recall that action `1` increments the agent's vertical speed while leaving the agent's horizontal speed unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "409bb221e1080a4a02e52db851d6dc86",
     "grade": false,
     "grade_id": "cell-8cb86c18bf331894",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAADrCAYAAAAv8oAHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFeElEQVR4nO3dsYpUZxjH4fcTV4S0qVOJewEBG9kLsLPRKwjb6A3Y5Sa0CFik1AUlVyDaCdrlEgRxDcQiW7iKJ0USEDLDzpmZ9fy/Oc8DFrp7eE/148x6vnfbMAwFkOLC1DcA8DVRAqKIEhBFlIAoogREESUgysWzvqG1dlhVh//+9cfzvR1gLoZhaIv+vY15T6m15qUmYCuWRcnHNyCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAlLFbAgDOlS0BMzf2t9m01ta6bpNrJ505emLVf0ffx1677nVTz1yXLQFAF0QJiCJKQBRRAqKIEhBFlIAoogREESUgiigBUUQJiCJKQBRRAqI4kAtMYtmBXKtLgCielIJ0tdJjLjNHT7S6ZFVWlwBdECUgiigBUUQJiCJKQBRRAqKIEhBFlIAoogREESUgiigBUZx9AyZhSwDQBU9KWzab0/NzmTl6oi0Bq7IlAOiCKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFAdygUlYXQJ0wZPSEl2t1zDz/GaOnmh1yaqsLgG6IEpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECojj7BkzClgCgCzv9pDSbk+wTzIRN2RIAdEGUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSDKTh/IBXJZXQJ0YaeflOayRmSTmTAVq0uALogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCg7ffYNyGVLANCFLp6U5nJif92Z0CNbAoAuiBIQRZSAKKIERBElIIooAVFECYgiSkAUUQKiiBIQRZSAKKIEROniQC6we6wuAbrQxZNST2tENpkJc2J1CdAFUQKiiBIQRZSAKKIERBElIIooAVFECYgiSkAUUQKiiBIQpYuzb8DusSUA6MI3e1Lq7cT+ujOB1dgSAHRBlIAoogREESUgiigBUUQJiCJKQBRRAqKIEhBFlIAoogREESUgitUlwCSsLgG6YHXJlmcCq7G6BOiCKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIYksAMAlbAoAu2BKw5ZnAamwJALogSkAUUQKiiBIQRZSAKLOL0vHxcd2581N9+vRp6lsBFphdlJ48eVIPHjysZ8+eTX0rwAKzi9LR0cM6OKh6/PjXqW8FWGBWL08eHx/X1as/1MuXH+v69e/q7ds/a29vb6szgdV4ebL++eh248bF2t+vunLlgo9wEGhWUTo6eli3bp1UVdXt23/5CAeBZhOl9+/f1/Pnr+vdu6qTk6oPH4Z6+vS3+vz589S3BnxlVqtLLl2qaq3q7t2q+/ervnypOj2d+q5gntb+mVJr7bC19qq19mr7t/VtnZ5WXb5cNQxVe3uCBIl2/H/f/v+1a9eqDg6q3rypevRo0bW10UxgNf73DeiCKAFRRAmIIkpAlFlG6fXrqe8AWObM32aya+7dq3rxourmzanvBFhkdq8EnH1tbTQTWI1XAoAuiBIQRZSAKKIERJnVlgAgx7IfdJ/5SkBr7bCqDrd+RwALfLsnpZ83uGbstetet42ZwEq8EgB0QZSAKKIERBElIIooAVFECYgiSkAUUQKiiBIQRZSAKKIERBElIIrVJcAkrC4BuuBJCZiE1SVAF0QJiCJKQBRRAqKIEhBFlIAoogREESUgiigBUUQJiCJKQBRRAqKM3RLwsap+X2PO91X1xxrXbXKtmWaamTtzf+lXhmFY+U9VvRrz/ZteZ6aZZs5vpo9vQBRRAqKMjdIva85Z9zozzTRzZjNHbZ4EOG8+vgFRRAmIIkpAFFECoogSEOVvLwOIJ7q9mM4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: (2, 3, 1, 0), Reward: -1, Terminal: False\n"
     ]
    }
   ],
   "source": [
    "# Let us increase the agent's vertical speed (action 1).\n",
    "next_state, reward, terminal = env.step(1)\n",
    "env.render()\n",
    "print(\"Next State: {}, Reward: {}, Terminal: {}\".format(next_state, reward, terminal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a68773faf856fac19dc863fe5e3b01f4",
     "grade": false,
     "grade_id": "cell-4f51e890424d0c2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can see that the agent has moved one square upwards, and now has a positive vertical speed (indicated by the yellow arrow). Let's set up a loop to see what happens if we take the action a few more times, causing it to repeatedly leave the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50931f73b836a9941366a8e8b67805a2",
     "grade": false,
     "grade_id": "cell-ef8865037a9ebdeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c5ba02feea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 50\n",
    "for t in range(num_steps) :\n",
    "    next_state, reward, terminal = env.step(1)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87d06e7642ead8dc7d6f126dda8742d3",
     "grade": false,
     "grade_id": "cell-a21a5643628d8354",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: On-Policy MC Control (6 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using On-Policy First-Visit MC Control, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 5.4 p.101).\n",
    "\n",
    "<img src=\"images/mc_control_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.15$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at **least** 20.\n",
    "\n",
    "**If you use incorrect parameters, you may not get any credit for your work.**\n",
    "\n",
    "Your implementation of a tabular **On-Policy First-Visit MC Control** agent should produce a list named `mc_rewards`. This list should contain one list for each agent that you train. Each sub-list should contain the undiscounted sum of rewards earned during each episode by the corresponding agent. <br />\n",
    "For example, if you train $20$ agents, your `mc_rewards` list will contain $20$ sub-lists, each containing $150$ integers. This list will be used to plot an average learning curve, which will be used to mark your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should implement your MC agent and plot your average learning curve here.\n",
    "import random\n",
    "from racetrack_env import RacetrackEnv\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "env = RacetrackEnv()\n",
    "\n",
    "# Initialise/reset environment.\n",
    "mc_rewards = []\n",
    "returns = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "policy = generatePolicy()\n",
    "Q = {}\n",
    "𝛾 = 0.9\n",
    "𝜖 = 0.15\n",
    "episodeNumber = 150\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a policy, return an episode\n",
    "def generateEpisode(policy):\n",
    "    currState = env.reset()\n",
    "    episode = []\n",
    "    reward = None\n",
    "    terminal = False\n",
    "    while terminal == False:\n",
    "        actionIndex = dictionaryRandomChoice(policy, currState)\n",
    "        episode.append([currState, actionIndex, reward])\n",
    "        #print(\"State: \", currState, \"Action: \", actionIndex)\n",
    "        currState, reward, terminal = env.step(actionIndex)\n",
    "    episode.append([currState, None, reward])\n",
    "    return episode\n",
    "\n",
    "def findMaxAction(Q, state):\n",
    "    QValues = []\n",
    "    for i in range(9):\n",
    "        if (state, i) in Q:\n",
    "            QValues.append(Q[state, i])\n",
    "        else:\n",
    "            QValues.append(-100)\n",
    "    bestAction = QValues.index(max(QValues))\n",
    "    return bestAction\n",
    "\n",
    "# goes through each action for a state in the dictionary and chooses an action based on probabilities\n",
    "def dictionaryRandomChoice(policy, state):\n",
    "    probs = []\n",
    "    for i in range (9):\n",
    "        probs.append(policy[state, i])\n",
    "    action = np.random.choice(9, p = probs)\n",
    "    #print(\"State: \", state, \"Probs: \", probs, \"Action: \", action )\n",
    "    return action\n",
    "\n",
    "def generatePolicy():\n",
    "    policy = {}\n",
    "    for x in range(1, 14):\n",
    "        for y in range(1, 18):\n",
    "            for speedx in range(-10, 11):\n",
    "                for speedy in range(-10, 11):\n",
    "                    for action in range(9):\n",
    "                        # set equal prob for each action at start\n",
    "                        policy[(x, y, speedx, speedy), action] = 1/9\n",
    "    return policy\n",
    "\n",
    "def checkExists(episode, state, action):\n",
    "    # remove reward from episode\n",
    "    newEpisode = np.asarray(episode)[0:t,[0, 1]]\n",
    "    for i in range (len(newEpisode)):\n",
    "        if newEpisode[i][0] == state and newEpisode[i][1] == action:\n",
    "            #print(\"Matched. State: \", state, \"Action: \", action, \"Index: \", i)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3c05d9fa9755221c2aaafee22235886",
     "grade": true,
     "grade_id": "cw2_racetrack_mc",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Episode  0\n",
      "Episode  10\n",
      "Episode  20\n",
      "Episode  30\n",
      "Episode  40\n",
      "Episode  50\n",
      "Episode  60\n",
      "Episode  70\n",
      "Episode  80\n",
      "Episode  90\n",
      "Episode  100\n",
      "Episode  110\n",
      "Episode  120\n",
      "Episode  130\n",
      "Episode  140\n",
      "[-1856, -99, -443, -609, -780, -1635, -1154, -991, -1311, -39, -630, -535, -303, -4701, -432, -222, -351, -122, -587, -52, -170, -175, -24, -36, -140, -45, -361, -439, -204, -23, -58, -81, -13, -108, -223, -94, -149, -299, -42, -58, -419, -28, -49, -82, -64, -213, -14, -112, -162, -182, -43, -284, -85, -252, -363, -47, -200, -136, -52, 3, -214, -307, -264, -10, -165, -103, -245, -159, -40, -150, -11, -70, -27, 1, 1, -218, -79, -31, -30, -16, -32, -320, -19, -135, -9, -370, -125, -1, -142, -146, -119, -116, -15, -95, -60, -191, -16, -44, -214, -373, -192, -416, 3, 3, -105, -34, -1, -73, -215, -104, -100, -189, -12, -239, -37, 1, -111, -101, -61, -188, 4, -78, -116, -69, -137, -32, -397, 4, -14, -43, 3, -273, -72, -14, -150, -70, -48, -240, -49, -27, -272, -27, -35, -276, 4, -151, -417, -21, -9, -221]\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting\")\n",
    "for currEpisode in range (episodeNumber):\n",
    "    if currEpisode % 10 == 0:\n",
    "        print(\"Episode \", currEpisode)\n",
    "    mc_rewards.append(0)\n",
    "    episode = generateEpisode(policy)\n",
    "    episodeList = []\n",
    "    G = 0\n",
    "    for t in range (len(episode) - 2, 0, -1):\n",
    "        state = episode[t][0]\n",
    "        action = episode[t][1]\n",
    "        reward = episode[t+1][2]\n",
    "        #print(\"State: \", state, \"Action: \", action, \"Reward: \", reward)\n",
    "        G = 𝛾 * G + reward\n",
    "        mc_rewards[currEpisode] += reward\n",
    "        if not checkExists(np.asarray(episode)[0:t,[0, 1]], state, action):\n",
    "            # add to returns list\n",
    "            returns[state, action] += G\n",
    "            N[state, action] += 1 \n",
    "            # find average\n",
    "            Q[state, action] = returns[state, action] / N[state, action]\n",
    "            actionIndex = findMaxAction(Q, state)\n",
    "            #print(\"State: \", state, \"Top action: \", actionIndex)\n",
    "            # go thru each action and check if matches, calculate probs\n",
    "            for i in range(9):\n",
    "                if i == actionIndex:\n",
    "                    policy[state, i] = 1 - 𝜖 + (𝜖/9)\n",
    "                else:\n",
    "                    policy[state, i] = 𝜖/9\n",
    "print(mc_rewards)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pprint.pprint(returns)\n",
    "#pprint.pprint(Q)\n",
    "#pprint.pprint(policy)\n",
    "plt.plot([i for i in range(0, episodeNumber)], mc_rewards)\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  -45\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "def playGame(policy):\n",
    "    state = env.reset()\n",
    "    episode = [state]\n",
    "    terminal = False\n",
    "    totalReward = 0\n",
    "    while terminal == False:\n",
    "        env.render()\n",
    "        actionIndex = dictionaryRandomChoice(policy, state)\n",
    "        state, reward, terminal = env.step(actionIndex)\n",
    "        totalReward += reward\n",
    "        episode.append(state)\n",
    "    return totalReward\n",
    "\n",
    "totalReward = playGame(policy)\n",
    "#pprint.pprint(policy)\n",
    "print(\"Reward: \", totalReward)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbd1e202cbe799b9a0c167c83f0cd0ed",
     "grade": false,
     "grade_id": "cell-be9d8a57636c5b72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-e02b4c50b67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Checking MC Control Results for Obvious Issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msimple_issue_checking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Plotting MC Control Learning Curve.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ReinforcedLearningAgents/rl_cw/racetrack_env.py\u001b[0m in \u001b[0;36msimple_issue_checking\u001b[0;34m(results, modified_agent)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# Check that all agents have been trained for 150 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Marking Advice] Agent {} was trained for {} episodes. Please train each of your agents for exactly 150 episodes.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "from racetrack_env import plot_results\n",
    "from racetrack_env import simple_issue_checking\n",
    "\n",
    "# Checking MC Control Results for Obvious Issues.\n",
    "simple_issue_checking(mc_rewards)\n",
    "\n",
    "# Plotting MC Control Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_results(mc_rewards = mc_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "773dcf30c08442ff48ae4852054e3e08",
     "grade": false,
     "grade_id": "cell-e74a2cb7d25f13ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Sarsa (3 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using the Sarsa algorithm, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.4 p.129).\n",
    "\n",
    "<img src=\"images/sarsa_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Step size parameter $\\alpha = 0.2$.\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.15$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at **least** 20.\n",
    "\n",
    "**If you use incorrect parameters, you may not get any credit for your work.**\n",
    "\n",
    "Your implementation of a tabular **Sarsa** agent should produce a list named `sarsa_rewards`. This list should contain one list for each agent that you train. Each sub-list should contain the undiscounted sum of rewards earned during each episode by the corresponding agent. <br />\n",
    "For example, if you train $20$ agents, your `sarsa_rewards` list will contain $20$ sub-lists, each containing $150$ integers. This list will be used to plot an average learning curve, which will be used to mark your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42f53184e1a71dd31c8b165837621f7",
     "grade": true,
     "grade_id": "cw2_racetrack_sarsa",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your sarsa agent and plot your average learning curve here.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3eeb47108f3f31849ecff073f5fe1ec",
     "grade": false,
     "grade_id": "cell-21e06572aedb066b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from racetrack_env import plot_results\n",
    "from racetrack_env import simple_issue_checking\n",
    "\n",
    "# Checking Sarsa Results for Obvious Issues.\n",
    "simple_issue_checking(sarsa_rewards)\n",
    "\n",
    "# Plotting Sarsa Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_results(sarsa_rewards = sarsa_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "541d57eb931a60e70338995642163b15",
     "grade": false,
     "grade_id": "cell-b0d45e347090cb02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Q-Learning (3 Marks)\n",
    "\n",
    "In this exercise, you will implement an agent which learns to reach a goal state in the racetrack task using the Q-Learning algorithm, the pseudocode for which is reproduced below (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131).\n",
    "\n",
    "<img src=\"images/q_learning_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Step size parameter $\\alpha = 0.2$.\n",
    "- Discount factor $\\gamma = 0.9$.\n",
    "- For your $\\epsilon$-greedy policy, use exploratory action probability $\\epsilon = 0.15$.\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at **least** 20.\n",
    "\n",
    "**If you use incorrect parameters, you may not get any credit for your work.**\n",
    "\n",
    "Your implementation of a tabular **Q-Learning** agent should produce a list named `q_learning_rewards`. This list should contain one list for each agent that you train. Each sub-list should contain the undiscounted sum of rewards earned during each episode by the corresponding agent. <br />\n",
    "For example, if you train $20$ agents, your `q_learning_rewards` list will contain $20$ sub-lists, each containing $150$ integers. This list will be used to plot an average learning curve, which will be used to mark your work.\n",
    "\n",
    "Hint: Your Q-Learning implementation is likely to be similar to your Sarsa implementation. Think hard about where these two algorithms differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95e47099546eb0affe62efff222a03bf",
     "grade": true,
     "grade_id": "cw2_racetrack_q_learning",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your q-learning agent agent and plot your average learning curve here.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68331dbf0440f04055cde4e6475b96bd",
     "grade": false,
     "grade_id": "cell-7a0093c066ad2572",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from racetrack_env import plot_results\n",
    "from racetrack_env import simple_issue_checking\n",
    "\n",
    "# Checking Q-Learning Results for Obvious Issues.\n",
    "simple_issue_checking(q_learning_rewards)\n",
    "\n",
    "# Plotting Q-Learning Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_results(q_learning_rewards = q_learning_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "941a43f8855e585150e50355ff001e49",
     "grade": false,
     "grade_id": "cell-b5dc1ef13af04d71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Comparison & Discussion (10 Marks)\n",
    "\n",
    "Below, we have used your results to plot your three previous learning curves on the same set of axes. <br />\n",
    "A cropped version of this learning curve has also been plotted, to make it easier to compare the performance of your agents towards the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d8596a71d3666f33192cb3161102bd1",
     "grade": false,
     "grade_id": "cw2_racetrack_comparison_graphs",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from racetrack_env import plot_combined_results\n",
    "\n",
    "# Plotting Combined Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_combined_results(mc_rewards, sarsa_rewards, q_learning_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a9004d76b6330cbe9870e5dc5e01b2d",
     "grade": false,
     "grade_id": "cell-20dcd6ae8c454970",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on your results, and your understanding of the algorithms that you have implemented, please answer the following discussion questions. <br />\n",
    "Please do not exceed **two sentences** for any of your answers.\n",
    "\n",
    "**Question 1:** Briefly compare the performance of each of your agents.\n",
    "\n",
    "**Question 2:** Why do you think that your Monte Carlo and Temporal-Difference agents behaved differently?\n",
    "\n",
    "**Question 3:** Does the performance of your Sarsa and Q-Learning agents meet your expectations? Why do you think that this was the case?\n",
    "\n",
    "**Question 4:** What could be done to improve the performance of your agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "668c64a27144d8e8a94fcd8e3918443c",
     "grade": true,
     "grade_id": "cw2_racetrack_discussion",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Please write your answers for Exercise 4 in this markdown cell.\n",
    "\n",
    "**Answer 1:**\n",
    "\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "\n",
    "**Answer 3:**\n",
    "\n",
    "\n",
    "**Answer 4:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "833e8de8d9e34fddbe7a93c89b06a315",
     "grade": false,
     "grade_id": "cell-4ec5137941b60726",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 5: Modified Q-Learning Agent (18 Marks)\n",
    "### Exercise 5a: Implementation\n",
    "In this exercise, you must implement a Temporal-Difference learning agent which learns to reach a goal state in the racetrack more efficiently than your previous Q-Learning agent. You may base your implementation on Q-Learning (Reinforcement Learning, Sutton & Barto, 2018, Section 6.5 p.131), the pseudocode for which is reproduced below, but you may also base your implementation on Sarsa if you wish.\n",
    "\n",
    "<img src=\"images/q_learning_algo.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "In order to score high marks in this exercise, you will need to extend your solution beyond a simple Q-Learning or Sarsa agent to achieve higher performance and/or more efficient learning (i.e. using fewer interactions with the environment). Ideas for improving your agent will have been discussed in the unit, and can be found in the course textbook (Reinforcement Learning, Sutton & Barto, 2018). However you go about improving your agent, it must still use a **tabular** Temporal-Difference learning method at its core.\n",
    "\n",
    "Please use the following parameter settings:\n",
    "- Number of training episodes $= 150$.\n",
    "- Number of agents averaged should be at **least** 2.\n",
    "\n",
    "**If you use incorrect parameters, you may not get any credit for your work.**\n",
    "\n",
    "You may adjust all other parameters as you see fit.\n",
    "\n",
    "\n",
    "Your implementation of a tabular modified Temporal-Difference learning agent should produce a list named `modified_agent_rewards`. This list should contain one list for each agent that you train. Each sub-list should contain the undiscounted sum of rewards earned during each episode by the corresponding agent. <br />\n",
    "For example, if you train $20$ agents, your `modified_agent_rewards` list will contain $20$ sub-lists, each containing $150$ integers. This list will be used to plot an average learning curve, which will be used to mark your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf3e77bdececb88b711efa0528848942",
     "grade": true,
     "grade_id": "cw2_racetrack_modified",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 5a in this cell or in as many cells as you want ABOVE this cell.\n",
    "# You should implement your modified q-learning agent agent and plot your average learning curve here.\n",
    "# Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19789e02924eab2fd1c9e061babd420a",
     "grade": false,
     "grade_id": "cell-d5827808ed0c8886",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 5b: Comparison & Discussion\n",
    "\n",
    "Below, we have used your results to plot a the performance of your modified agent and your previous Q-Learning agent on the same set of axes. <br />\n",
    "A cropped version of this learning curve has also been plotted, to make it easier to compare the performance of your agents towards the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7749344d652fae9a366cfc5309b5fa2",
     "grade": false,
     "grade_id": "cw2_racetrack_modified_comparison_graphs",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from racetrack_env import plot_modified_agent_results\n",
    "from racetrack_env import simple_issue_checking\n",
    "\n",
    "# Checking Modified Agent Results for Obvious Issues.\n",
    "simple_issue_checking(modified_agent_rewards, modified_agent = True)\n",
    "\n",
    "# Plotting Modified Agent Learning Curve.\n",
    "%matplotlib inline\n",
    "plot_modified_agent_results(q_learning_rewards, modified_agent_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "044ed5bb5e932225233c282b16530706",
     "grade": false,
     "grade_id": "cell-877eaf5400c160bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on your results, and your understanding of the algorithm and modifications that you have implemented, please answer the following discussion questions. <br />\n",
    "Please do not exceed **two sentences** for any of your answers.\n",
    "\n",
    "**Question 1:** What modifications did you make to your agent?\n",
    "\n",
    "**Question 2:** What effect(s) did you expect your modifications to have on the performance of your agent?\n",
    "\n",
    "**Question 3:** Did your modifications have the effect(s) you expected? Why do you think that this was the case?\n",
    "\n",
    "**Question 4:** If you had more time, what would you do to further improve the performance of your agent?\n",
    "\n",
    "Please note that **your implementation and discussion will be assessed jointly**. This means that, in order to score highly, you will need to correctly implement appropriate modifications to your agent **AND** discuss them well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acfda8ff076779eeaf6824ffe2970876",
     "grade": true,
     "grade_id": "cw2_racetrack_modified_discussion",
     "locked": false,
     "points": 18,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Please write your answers for Exercise 5b in this markdown cell.\n",
    "\n",
    "**Answer 1:**\n",
    "\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "\n",
    "**Answer 3:**\n",
    "\n",
    "\n",
    "**Answer 4:**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
